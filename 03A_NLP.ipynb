{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP) in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings, or word vectors, provide a way of mapping words from a vocabulary into a low-dimensional space, where words with similar meanings are close together. Let's play around with a set of pre-trained word vectors, to get used to their properties. There exist many sets of pretrained word embeddings; here, we use ConceptNet Numberbatch, which provides a relatively small download in an easy-to-work-with format (h5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download word vectors\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "if not os.path.isfile('mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'mini.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read an `h5` file, we'll need to use the `h5py` package. Below, we use the package to open the `mini.h5` file we just downloaded. We extract from the file a list of utf-8-encoded words, as well as their $300$-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 362891\n",
      "all_embeddings dimensions: (362891, 300)\n",
      "/c/de/lande\n"
     ]
    }
   ],
   "source": [
    "# Load the file and pull out words and embeddings\n",
    "import h5py\n",
    "with h5py.File('mini.h5', 'r') as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "print(\"all_words dimensions: {0}\".format(len(all_words)))\n",
    "print(\"all_embeddings dimensions: {0}\".format(all_embeddings.shape))\n",
    "\n",
    "print(all_words[10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `all_words` is a list of $V$ strings (what we call our *vocabulary*), and `all_embeddings` is a $V \\times 300$ matrix. The strings are of the form `/c/language_code/word`â€”for example, `/c/en/cat` and `/c/es/gato`.\n",
    "\n",
    "We are interested only in the English words. We use Python list comprehensions to pull out the indices of the English words, then extract just the English words (stripping the six-character `/c/en/` prefix) and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 150875\n",
      "all_embeddings dimensions: (150875, 300)\n",
      "bajillion\n"
     ]
    }
   ],
   "source": [
    "# Restrict our vocabulary to just the English words\n",
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en/')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en/')]\n",
    "english_embeddings = all_embeddings[english_word_indices]\n",
    "\n",
    "print(\"all_words dimensions: {0}\".format(len(english_words)))\n",
    "print(\"all_embeddings dimensions: {0}\".format(english_embeddings.shape))\n",
    "\n",
    "print(english_words[10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look up words easily, so we create a dictionary that maps us from a word to its index in the word embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {word: i for i, word in enumerate(english_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word has an associated 300-dimensional embedding, which we can directly show by calling the word embeddings with that dictionary word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat has word index 21398\n",
      "cat has word embedding [  0   0   2  -5   1   0  -1  -3   3  -2   2   1   4   0  -3   0  -1  -4\n",
      "   0   1   0  -2   0  -2  -2   3   1  -2  -2   3  -2   1   0   0  -3   1\n",
      "  -4   6  -1   2  -4   4  -3   0  -2  -3  -1  -5   5  -6   2   2  -3   0\n",
      "  -1   4  -1   2   2   1   3   1   3  -1  -8  -3   2   0   1   4  -1  -5\n",
      "   2   0  -5   1   0  -7  -4   2   3   4   3   3  -4   8   0   4  -2   0\n",
      "   3   1  -4   0   3   0  -2   0  -1  -8   0   0   2   0   2   4   7   3\n",
      "  -2   3  -3   3   2  -3  -5   0  -2   2   0  -3  10  -2   0  -3  -2  -5\n",
      "   4   0   0   3   2  -1  -1   5  -4  -2   0  -2  -8   0  -4   4   9  -6\n",
      "   5   4  -7  -1  -2  -3   2   4   0  -1  -6  -2  -4  -4  -2   0   4  -5\n",
      "   0   2   0   4   1   3   7  -3   4   0   6   0 -12  -4   0  -2  -6   1\n",
      "   6  -5   4   0   2  -6   2  -2   2   3  10   0   0   4   4  -2   0  -2\n",
      "   0   3   1   0  -9   3   3  -6   0   0   0  -8   1   5   2  -2   0   1\n",
      "  -3   4  -5  -5   3   1   4  -4   0   2  -2  -1  -4  -1   6  -2   3   3\n",
      "  -1  -1   2  -2   0   4   0   2   1  -2   0   1   0  -2   5  -9  -4   1\n",
      "   4   2   0   1   0  -3   1  -8   7   3  -4   3  -2   1  -1   2   0  -1\n",
      "   2   0   0  -2   2   1   4  -1   1  -2   6  -2   4  -1   0   1  -2  -1\n",
      "   2   0   3  -1   4   4   4   0   1   0  -1   0]\n"
     ]
    }
   ],
   "source": [
    "our_word='cat'\n",
    "our_word_index=index['cat']\n",
    "our_embedding=english_embeddings[our_word_index]\n",
    "print(our_word,'has word index',our_word_index)\n",
    "print(our_word, 'has word embedding',our_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of a word vector is less important than its direction; the magnitude can be thought of as representing frequency of use, independent of the semantics of the word. \n",
    "Here, we will be interested in semantics, so we *normalize* our vectors, dividing each by its length. \n",
    "The result is that all of our word vectors are length 1, and as such, lie on a unit circle. \n",
    "The dot product of two vectors is proportional to the cosine of the angle between them, and provides a measure of similarity (the bigger the cosine, the smaller the angle).\n",
    "\n",
    "<img src=\"Figures/cosine_similarity.png\" alt=\"cosine\" style=\"width: 500px;\"/>\n",
    "<center>Figure adapted from *[Mastering Machine Learning with Spark 2.x](https://www.safaribooksonline.com/library/view/mastering-machine-learning/9781785283451/ba8bef27-953e-42a4-8180-cea152af8118.xhtml)*</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "norms = np.linalg.norm(english_embeddings, axis=1)\n",
    "normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to measure the similarity between pairs of words. We use numpy to take dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1], :], normalized_embeddings[index[w2], :])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\tcat\t 1.0\n",
      "cat\tfeline\t 0.8199548\n",
      "cat\tdog\t 0.590724\n",
      "cat\tmoo\t 0.0039538248\n",
      "cat\tfreeze\t -0.030225184\n",
      "antonyms\topposites\t 0.3941065\n",
      "antonyms\tsynonyms\t 0.46883982\n"
     ]
    }
   ],
   "source": [
    "# A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
    "\n",
    "# Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
    "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
    "\n",
    "# Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
    "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))\n",
    "\n",
    "# Antonyms are still considered related, sometimes more so than synonyms\n",
    "print('antonyms\\topposites\\t', similarity_score('antonym', 'opposite'))\n",
    "print('antonyms\\tsynonyms\\t', similarity_score('antonym', 'synonym'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find, for instance, the most similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to_vector(v, n):\n",
    "    all_scores = np.dot(normalized_embeddings, v)\n",
    "    best_words = map(lambda i: english_words[i], reversed(np.argsort(all_scores)))\n",
    "    return [next(best_words) for _ in range(n)]\n",
    "\n",
    "def most_similar(w, n):\n",
    "    return closest_to_vector(normalized_embeddings[index[w], :], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'humane_society', 'kitten', 'feline', 'colocolo', 'cats', 'kitty', 'maine_coon', 'housecat', 'sharp_teeth']\n",
      "['dog', 'dogs', 'wire_haired_dachshund', 'doggy_paddle', 'lhasa_apso', 'good_friend', 'puppy_dog', 'bichon_frise', 'woof_woof', 'golden_retrievers']\n",
      "['duke', 'dukes', 'duchess', 'duchesses', 'ducal', 'dukedom', 'duchy', 'voivode', 'princes', 'prince']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar('cat', 10))\n",
    "print(most_similar('dog', 10))\n",
    "print(most_similar('duke', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you explain the following similarity scores?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8478777"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"sit\", \"sits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858501"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"want\", \"wants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8664926"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"sleep\", \"sleeps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42647985"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_score(\"leave\", \"leaves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you find a polysemous word â€” a word with multiple meanings â€” so that the list of the top 10 most related words contains words that aren't themselves related to one another?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leave', 'leaving', 'come_away', 'depart', 'beleave', 'go_forth', 'quit', 'go_away', 'vacate', 'departing']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar('leave', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leaves', 'leaf', 'foliage', 'banana_leaf', 'leaflike', 'leaved', 'betel_leaf', 'thai_basil', 'oak_trees', 'aspidistra']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar('leaves', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['solution', 'solutions', 'aqueous_solution', 'subproblem', 'solve', 'aqueous_phase', 'virtuous_circle', 'solves', 'resolvent', 'solving']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar('solution', 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `closest_to_vector` to find words \"nearby\" vectors that we create ourselves. This allows us to solve analogies. For example, in order to solve the analogy \"man : brother :: woman : ?\", we can compute a new vector `brother - man + woman`: the meaning of brother, minus the meaning of man, plus the meaning of woman. We can then ask which words are closest, in the embedding space, to that new vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_analogy(a1, b1, a2):\n",
    "    b2 = normalized_embeddings[index[b1], :] - normalized_embeddings[index[a1], :] + normalized_embeddings[index[a2], :]\n",
    "    return closest_to_vector(b2, 5)\n",
    "def print_analogy(a1, b1,a2):\n",
    "    closest_words=solve_analogy(a1,b1,a2)\n",
    "    print(\"{0}:{1} as {2}:?\".format(a1,b1,a2))\n",
    "    print(\"Best guesses are: {}\".format(closest_words))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:brother as woman:?\n",
      "Best guesses are: ['sister', 'brother', 'sisters', 'kid_sister', 'younger_brother']\n",
      "man:husband as woman:?\n",
      "Best guesses are: ['wife', 'husband', 'husbands', 'spouse', 'wifes']\n",
      "spain:madrid as france:?\n",
      "Best guesses are: ['paris', 'france', 'le_havre', 'in_france', 'montmartre']\n",
      "dog:golden_retriever as cat:?\n",
      "Best guesses are: ['cat', 'maine_coon', 'kitten', 'tabby', 'kitty']\n"
     ]
    }
   ],
   "source": [
    "print_analogy(\"man\", \"brother\", \"woman\")\n",
    "print_analogy(\"man\", \"husband\", \"woman\")\n",
    "print_analogy(\"spain\", \"madrid\", \"france\")\n",
    "print_analogy(\"dog\", \"golden_retriever\", \"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the new vector, `b2`, is not normalized. Does this matter?  Why or why not?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three results are quite good, but in general, the results of these analogies can be disappointing. Try experimenting with other analogies, and see if you can think of ways to get around the problems you notice (i.e., modifications to the solve_analogy algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word embeddings in deep models\n",
    "Word embeddings are fun to play around with, but their primary use is that they allow us to think of words as existing in a continuous, Euclidean space; we can then use an existing arsenal of techniques for machine learning with continuous numerical data (like logistic regression or neural networks) to process text.\n",
    "\n",
    "Let's take a look at an especially simple version of this. We'll perform *sentiment analysis* on a set of movie reviews: in particular, we will attempt to classify a movie review as positive or negative based on its text.\n",
    "\n",
    "We will use a simplified version of [Simple Word Embedding Model](http://people.ee.duke.edu/~lcarin/acl2018_swem.pdf) (SWEM, Shen et al. 2018) to do so. We will represent a review as the *mean* of the embeddings of the words in the review (SWEM would also update the word embeddings). Then we'll train a three-layer MLP (a neural network) to classify the review as positive or negative.\n",
    "\n",
    "Download the `movie-simple.txt` file from Google Classroom into this directory. Each line of that file contains \n",
    "\n",
    "1. the numeral 0 (for negative) or the numeral 1 (for positive), followed by\n",
    "2. a tab (the whitespace character), and then\n",
    "3. the review itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    \n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    \n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    \n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "with open(\"Resources/movie-simple.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1411"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset, let's shuffle it and do a train/test split. We use a quarter of the dataset for testing, 3/4 for training (but also ensure that we have a whole number of batches in our training set, to make the code nicer later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)\n",
    "\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3*total_batches // 4 \n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our MLP in Tensorflow. We'll use placeholders for `X` and `y` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0707 14:08:54.968827 4324869568 deprecation.py:506] From /anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0707 14:08:55.023981 4324869568 deprecation.py:323] From /anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, 300])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.keras.layers.Dense(100, activation='relu')(X)\n",
    "h2 = tf.keras.layers.Dense(20, activation='relu')(h1)\n",
    "logits = tf.keras.layers.Dense(1)(h2)\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(probabilities), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin a session and train our model. We'll train for 250 epochs. When we're finished, we'll evaluate our accuracy on all the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Loss: 0.694661021232605 \t Acc: 0.46000000834465027\n",
      "Epoch: 10 \t Loss: 0.6678881645202637 \t Acc: 0.6000000238418579\n",
      "Epoch: 20 \t Loss: 0.667625904083252 \t Acc: 0.5899999737739563\n",
      "Epoch: 30 \t Loss: 0.6497185230255127 \t Acc: 0.6000000238418579\n",
      "Epoch: 40 \t Loss: 0.6203657388687134 \t Acc: 0.699999988079071\n",
      "Epoch: 50 \t Loss: 0.6083799600601196 \t Acc: 0.6700000166893005\n",
      "Epoch: 60 \t Loss: 0.5409393906593323 \t Acc: 0.800000011920929\n",
      "Epoch: 70 \t Loss: 0.4703376889228821 \t Acc: 0.8799999952316284\n",
      "Epoch: 80 \t Loss: 0.4446973502635956 \t Acc: 0.8899999856948853\n",
      "Epoch: 90 \t Loss: 0.3350175619125366 \t Acc: 0.9300000071525574\n",
      "Epoch: 100 \t Loss: 0.2757123112678528 \t Acc: 0.9100000262260437\n",
      "Epoch: 110 \t Loss: 0.2119666039943695 \t Acc: 0.9599999785423279\n",
      "Epoch: 120 \t Loss: 0.22098800539970398 \t Acc: 0.9200000166893005\n",
      "Epoch: 130 \t Loss: 0.20852069556713104 \t Acc: 0.9200000166893005\n",
      "Epoch: 140 \t Loss: 0.18307356536388397 \t Acc: 0.9399999976158142\n",
      "Epoch: 150 \t Loss: 0.13907337188720703 \t Acc: 0.9800000190734863\n",
      "Epoch: 160 \t Loss: 0.15098930895328522 \t Acc: 0.9399999976158142\n",
      "Epoch: 170 \t Loss: 0.12082812935113907 \t Acc: 0.949999988079071\n",
      "Epoch: 180 \t Loss: 0.13609758019447327 \t Acc: 0.9800000190734863\n",
      "Epoch: 190 \t Loss: 0.11860717087984085 \t Acc: 0.9599999785423279\n",
      "Epoch: 200 \t Loss: 0.10214719921350479 \t Acc: 0.9800000190734863\n",
      "Epoch: 210 \t Loss: 0.09603926539421082 \t Acc: 0.9700000286102295\n",
      "Epoch: 220 \t Loss: 0.06369809806346893 \t Acc: 0.9900000095367432\n",
      "Epoch: 230 \t Loss: 0.07995471358299255 \t Acc: 0.9800000190734863\n",
      "Epoch: 240 \t Loss: 0.09792668372392654 \t Acc: 0.9800000190734863\n",
      "Final accuracy: 0.9318735003471375\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "for epoch in range(250):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1,1])\n",
    "        \n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: {0} \\t Loss: {1} \\t Acc: {2}\".format(epoch, l, acc))\n",
    "    \n",
    "    random.shuffle(train)\n",
    "        \n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels  = np.array(test_labels).reshape([-1, 1])\n",
    "\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "print(\"Final accuracy: {0}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine what our model has learned, seeing how it responds to word vectors for different words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exciting [[0.99996054]]\n",
      "hated [[0.]]\n",
      "boring [[9.049776e-07]]\n",
      "loved [[0.99999857]]\n",
      "extremely [[0.5834872]]\n",
      "rather [[0.04839796]]\n",
      "quite [[0.98400116]]\n"
     ]
    }
   ],
   "source": [
    "# Check some words\n",
    "words_to_test = [\"exciting\", \"hated\", \"boring\", \"loved\", \"extremely\", \"rather\", \"quite\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    print(word, sess.run(probabilities, feed_dict={X: normalized_embeddings[index[word]].reshape(1, 300)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some words of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works great for such a simple dataset, but does a little less well on something more complex. `movie-pang02.txt`, for instance, has 2000 longer, more complex movie reviews. It's in the same format as our simple dataset. On those longer reviews, this model achieves only 60-80% accuracy. (Increasing the number of epochs to, say, 1000, does help.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show this in practice, this is a condensed version of the same code above working on this second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss 0.69329107 Acc 0.5\n",
      "Epoch 10 Loss 0.69512314 Acc 0.43\n",
      "Epoch 20 Loss 0.6928453 Acc 0.5\n",
      "Epoch 30 Loss 0.6915968 Acc 0.64\n",
      "Epoch 40 Loss 0.6911556 Acc 0.58\n",
      "Epoch 50 Loss 0.69427454 Acc 0.45\n",
      "Epoch 60 Loss 0.6915856 Acc 0.54\n",
      "Epoch 70 Loss 0.6909847 Acc 0.56\n",
      "Epoch 80 Loss 0.6919686 Acc 0.5\n",
      "Epoch 90 Loss 0.69048 Acc 0.72\n",
      "Epoch 100 Loss 0.69125223 Acc 0.58\n",
      "Epoch 110 Loss 0.69396216 Acc 0.41\n",
      "Epoch 120 Loss 0.689751 Acc 0.55\n",
      "Epoch 130 Loss 0.6901834 Acc 0.63\n",
      "Epoch 140 Loss 0.68938124 Acc 0.65\n",
      "Epoch 150 Loss 0.6890285 Acc 0.7\n",
      "Epoch 160 Loss 0.6888389 Acc 0.56\n",
      "Epoch 170 Loss 0.6890057 Acc 0.6\n",
      "Epoch 180 Loss 0.68678284 Acc 0.72\n",
      "Epoch 190 Loss 0.6907995 Acc 0.56\n",
      "Epoch 200 Loss 0.6856157 Acc 0.62\n",
      "Epoch 210 Loss 0.6854085 Acc 0.74\n",
      "Epoch 220 Loss 0.6849405 Acc 0.73\n",
      "Epoch 230 Loss 0.689507 Acc 0.57\n",
      "Epoch 240 Loss 0.6877799 Acc 0.56\n",
      "Final accuracy: 0.686\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each line in the file.\n",
    "with open(\"Resources/movie-pang02.txt\", \"r\",encoding='utf-8') as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]\n",
    "import random\n",
    "random.shuffle(dataset)\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]\n",
    "sess = tf.Session()\n",
    "initialize_all = tf.global_variables_initializer() \n",
    "sess.run(initialize_all)\n",
    "for epoch in range(250):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "print(\"Final accuracy:\", acc)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not plan to go through Recurrent Neural Networks in detail, but below is some example code on RNNs to provide some guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In the context of deep learning, natural language is commonly modeled with Recurrent Neural Networks (RNNs).\n",
    "RNNs pass the output of a neuron back to the input of the next time step of the same neuron.\n",
    "These directed cycles in the RNN architecture gives them the ability to model temporal dynamics, making them particularly suited for modeling sequences (e.g. text).\n",
    "We can visualize an RNN layer as follows:\n",
    "\n",
    "<img src=\"Figures/basic_RNN.PNG\" alt=\"basic_RNN\" style=\"width: 80px;\"/>\n",
    "<center>Figure from *Understanding LSTMs*. https://colah.github.io/posts/2015-08-Understanding-LSTMs/</center>\n",
    "\n",
    "We can unroll an RNN through time, making the sequence aspect of them more obvious:\n",
    "\n",
    "<img src=\"Figures/unrolled_RNN.PNG\" alt=\"basic_RNN\" style=\"width: 400px;\"/>\n",
    "<center>Figure from *Understanding LSTMs*. https://colah.github.io/posts/2015-08-Understanding-LSTMs/</center>\n",
    "\n",
    "#### RNNs in TensorFlow\n",
    "How would we implement an RNN in TensorFlow? Given the different forms of RNNs, there are quite a few ways, but we'll stick to a simple one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are dealing with a Recurrent Neural Network, we can have each word be a separate input to the network.  Given our word embeddings, that will be given by a matrix. The preprocessing pipeline will be slightly different than it was before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y, w), where x is 300-dimensional representation\n",
    "# of the words in a review,  y is its label, and w is the concatenated word embeddings\n",
    "def convert_line_to_example_rnn(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y, 'w':embeddings, 'text':line[2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Resources/movie-simple.txt\", \"r\") as f:\n",
    "    dataset = [convert_line_to_example_rnn(l) for l in f.readlines()]\n",
    "import random\n",
    "random.seed(200)\n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DaVinci Code and Mission Impossible 3 are AWESOME.\n",
      "\n",
      "1\n",
      "(300,)\n",
      "(1, 9, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train[0]['text'])\n",
    "print(train[0]['y'])\n",
    "print(train[0]['x'].shape)\n",
    "print(np.array(train[0]['w']).reshape([1,-1,300]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying an RNN to the text reviews, starting with the easier data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0707 14:09:13.951710 4324869568 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0707 14:09:13.952934 4324869568 deprecation.py:323] From <ipython-input-31-6895aaacaaf8>:9: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0707 14:09:13.954792 4324869568 deprecation.py:323] From <ipython-input-31-6895aaacaaf8>:10: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0707 14:09:14.053740 4324869568 deprecation.py:506] From /anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0707 14:09:14.277361 4324869568 deprecation.py:323] From <ipython-input-31-6895aaacaaf8>:12: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# sizes\n",
    "n_steps = None\n",
    "n_inputs = 300\n",
    "n_neurons = 5\n",
    "# Build RNN\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 499 Loss 0.71062994 Acc 0.0\n",
      "batch 999 Loss 0.25448325 Acc 1.0\n",
      "Epoch 0 Loss 0.19355781 Acc 1.0\n",
      "batch 499 Loss 0.10916404 Acc 1.0\n",
      "batch 999 Loss 0.06045605 Acc 1.0\n",
      "Epoch 1 Loss 0.08459929 Acc 1.0\n",
      "batch 499 Loss 0.04655488 Acc 1.0\n",
      "batch 999 Loss 0.08473836 Acc 1.0\n",
      "Epoch 2 Loss 0.691367 Acc 1.0\n",
      "batch 499 Loss 0.011898962 Acc 1.0\n",
      "batch 999 Loss 0.029992454 Acc 1.0\n",
      "Epoch 3 Loss 1.4959812 Acc 0.0\n",
      "batch 499 Loss 0.004806321 Acc 1.0\n",
      "batch 999 Loss 0.018642053 Acc 1.0\n",
      "Epoch 4 Loss 0.07946477 Acc 1.0\n",
      "batch 499 Loss 0.042612962 Acc 1.0\n",
      "batch 999 Loss 0.026600048 Acc 1.0\n",
      "Epoch 5 Loss 0.01851268 Acc 1.0\n",
      "batch 499 Loss 0.42374453 Acc 1.0\n",
      "batch 999 Loss 0.06636396 Acc 1.0\n",
      "Epoch 6 Loss 0.0114364475 Acc 1.0\n",
      "batch 499 Loss 0.071039006 Acc 1.0\n",
      "batch 999 Loss 2.0734136 Acc 0.0\n",
      "Epoch 7 Loss 0.012908939 Acc 1.0\n",
      "batch 499 Loss 0.53614306 Acc 1.0\n",
      "batch 999 Loss 0.0009447867 Acc 1.0\n",
      "Epoch 8 Loss 0.0026357113 Acc 1.0\n",
      "batch 499 Loss 0.007646476 Acc 1.0\n",
      "batch 999 Loss 4.2392597 Acc 0.0\n",
      "Epoch 9 Loss 0.0016768145 Acc 1.0\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(10):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([batch_size,-1,300])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([batch_size,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        if (batch+1) % 500 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l, \"Acc\", acc)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9546742209631728\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching to an LSTM-based neural network is fairly easy in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0707 14:09:45.469296 4324869568 deprecation.py:323] From <ipython-input-36-17e30b7b2cf4>:9: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# sizes\n",
    "n_steps = None\n",
    "n_inputs = 300\n",
    "n_neurons = 5\n",
    "# Build RNN\n",
    "X= tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y= tf.placeholder(tf.float32, [None, 1])\n",
    "basic_cell = tf.contrib.rnn.LSTMCell(n_neurons,activation=tf.nn.tanh)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)\n",
    "last_cell_output=outputs[:,-1,:]\n",
    "y_=tf.layers.dense(last_cell_output,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and metrics\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_, labels=y))\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_)), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.AdamOptimizer(0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 499 Loss 0.5775424 Acc 1.0\n",
      "batch 999 Loss 0.24180458 Acc 1.0\n",
      "Epoch 0 Loss 0.24794434 Acc 1.0\n",
      "batch 499 Loss 0.113387376 Acc 1.0\n",
      "batch 999 Loss 0.068844244 Acc 1.0\n",
      "Epoch 1 Loss 0.04553669 Acc 1.0\n",
      "batch 499 Loss 0.10069063 Acc 1.0\n",
      "batch 999 Loss 0.05680757 Acc 1.0\n",
      "Epoch 2 Loss 0.07640334 Acc 1.0\n",
      "batch 499 Loss 0.0638089 Acc 1.0\n",
      "batch 999 Loss 0.039913934 Acc 1.0\n",
      "Epoch 3 Loss 0.13318259 Acc 1.0\n",
      "batch 499 Loss 0.11254047 Acc 1.0\n",
      "batch 999 Loss 0.011101064 Acc 1.0\n",
      "Epoch 4 Loss 0.060186535 Acc 1.0\n",
      "batch 499 Loss 0.37860316 Acc 1.0\n",
      "batch 999 Loss 0.024089823 Acc 1.0\n",
      "Epoch 5 Loss 0.006876581 Acc 1.0\n",
      "batch 499 Loss 0.027292065 Acc 1.0\n",
      "batch 999 Loss 0.011620495 Acc 1.0\n",
      "Epoch 6 Loss 0.0006780038 Acc 1.0\n",
      "batch 499 Loss 0.03335786 Acc 1.0\n",
      "batch 999 Loss 0.006775183 Acc 1.0\n",
      "Epoch 7 Loss 0.0286195 Acc 1.0\n",
      "batch 499 Loss 0.0084675625 Acc 1.0\n",
      "batch 999 Loss 0.0048526647 Acc 1.0\n",
      "Epoch 8 Loss 0.0012415291 Acc 1.0\n",
      "batch 499 Loss 0.012337194 Acc 1.0\n",
      "batch 999 Loss 0.0023873444 Acc 1.0\n",
      "Epoch 9 Loss 0.0040586023 Acc 1.0\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(10):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([batch_size,-1,300])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([batch_size,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        if (batch+1) % 500 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l, \"Acc\", acc)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.9858356940509915\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, swapping out to a more complex dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Resources/movie-pang02.txt\", \"r\") as f:\n",
    "    dataset = [convert_line_to_example_rnn(l) for l in f.readlines()]\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "batch_size = 1\n",
    "total_batches = len(dataset) // batch_size\n",
    "train_batches = 3 * total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining one of the reviews, we can see that these are much more complex reviews, and requiring greater nuance to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the central focus of michael winterbottom s   welcome to sarajevo   is sarajevo itself   the city under siege   and its different effect on the characters unfortunate enough to be stuck there    it proves the backdrop for a stunningly realized story which refreshingly strays from mythic portents     platoon       racial tumultuosness   the risible   the walking dead     or a tinge of schmaltziness     schindler s list        the two leads   stephen dillane as a reporter and emira nusevic as an orphan with a plight few can identify with   are extremely believable   not one moment with them involved rings false    the question is not what went right    the question is what went wrong    for one   the film fails to provide a political overview of the war as it progresses   the dillane characters reports an american plane departing from sarajevo as it departs   and that s about it        the assortment of high profile supporting actors   ranging from woody harrelson as a yankee reporter   into   liquor and cigarrettes to marisa tomei as a huggable children s aid or somesuch are incapable of rising above the sketchiness of their characters   albeit they strive    the interrupted use of authentic war footage somewhat hampers the rest of the film   it makes the fictional characters seem powerless by comparison    still   winterbottom eschews mawkishness through flashy   frantic editing and imaginative use of music    and it s a plus   because he doesn t toy with our emotions with sentimental blandness    he wants us to know that in war   no one is victorious    the sweet hereafter  starring ian holm   sarah polley   bruce greenwood   tom mccamus   gabrielle rose   arsinee khanjian   alberta watson   maury chaykin   caerthan banks    produced by atom egoyan and camelia frieberg    script by atom egoyan   based on the novel by russell banks      directed by atom egoyan    running time   110 mins    rated r                                                               atom egoyan s powerfully meditative   the sweet hereafter   is as anything as haunting and transcendental as i ve seen this year    it not only explores the aftermaths of a terrible tragedy with magnificent subtlety   with cold   and stunning shots of the backdrop of this mistfortune but also by telegraphing the sense of devastation that has permeated the small town with an enormous amount of dignity and respect for each and every soul affected    ian holm as the somber   lonely lawyer seeking compensation for the townspeople   has a decency and a restraint uncommon with lawyers   he s a lion for the wrong reasons   the pain for having lost his daughter   she s a druggie who frequently calls him begging for money   is reflected on the town    and in some mesmerizing flashbacks   we witness how she lost her purity    it s one of the most strikingly breathtaking takes on the loss of innocence i ve ever seen    but the most astounding part goes to sarah polly   the peaceful   benevolent girl who   in contrast to the other characters   remains the most mentally stable during past   present or future    her ability to convey concealed pain and unconditional love is the opposite   yet near revolutionary role that won emily watson an oscar nomination for   breaking the waves      both women are torn apart by an extraordinary incident but it is only then that we truly see   truly feel   their wordless   omnipotent love       the sweet hereafter   though   is atom egoyan s movie and as of now   he is emerging as the definite face of independent cinema    after surging to worldwide recognition with 1994 s   exotica       the sweet hereafter   makes him something few people in this industry can call themselves   an artist    he interweaves time with a delicacy and sees a soul with a purity that is just not common    \n",
      "\n",
      "1\n",
      "(300,)\n",
      "(1, 577, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train[0]['text'])\n",
    "print(train[0]['y'])\n",
    "print(train[0]['x'].shape)\n",
    "print(np.array(train[0]['w']).reshape([1,-1,300]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrain on the same LSTM that we were using before. Note that this is _much_ slower because the RNN now has to deal with much longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 499 Loss 0.60423577 Acc 1.0\n",
      "batch 999 Loss 0.654327 Acc 1.0\n",
      "batch 1499 Loss 0.6145861 Acc 1.0\n",
      "Epoch 0 Loss 0.6145861 Acc 1.0\n",
      "batch 499 Loss 0.6395343 Acc 1.0\n",
      "batch 999 Loss 0.6404175 Acc 1.0\n",
      "batch 1499 Loss 0.52117807 Acc 1.0\n",
      "Epoch 1 Loss 0.52117807 Acc 1.0\n",
      "batch 499 Loss 0.91462994 Acc 0.0\n",
      "batch 999 Loss 0.26005527 Acc 1.0\n",
      "batch 1499 Loss 0.6265969 Acc 1.0\n",
      "Epoch 2 Loss 0.6265969 Acc 1.0\n",
      "batch 499 Loss 0.078632645 Acc 1.0\n",
      "batch 999 Loss 1.0986214 Acc 0.0\n",
      "batch 1499 Loss 0.38152325 Acc 1.0\n",
      "Epoch 3 Loss 0.38152325 Acc 1.0\n",
      "batch 499 Loss 0.19007088 Acc 1.0\n",
      "batch 999 Loss 0.33761886 Acc 1.0\n",
      "batch 1499 Loss 0.10387851 Acc 1.0\n",
      "Epoch 4 Loss 0.10387851 Acc 1.0\n",
      "batch 499 Loss 0.35062757 Acc 1.0\n",
      "batch 999 Loss 0.52068454 Acc 1.0\n",
      "batch 1499 Loss 0.34132633 Acc 1.0\n",
      "Epoch 5 Loss 0.34132633 Acc 1.0\n",
      "batch 499 Loss 0.2740763 Acc 1.0\n",
      "batch 999 Loss 0.15343344 Acc 1.0\n",
      "batch 1499 Loss 0.11593928 Acc 1.0\n",
      "Epoch 6 Loss 0.11593928 Acc 1.0\n",
      "batch 499 Loss 0.27533206 Acc 1.0\n",
      "batch 999 Loss 0.30982345 Acc 1.0\n",
      "batch 1499 Loss 1.1310893 Acc 0.0\n",
      "Epoch 7 Loss 1.1310893 Acc 0.0\n",
      "batch 499 Loss 0.28048468 Acc 1.0\n",
      "batch 999 Loss 2.824692 Acc 0.0\n",
      "batch 1499 Loss 0.98897755 Acc 0.0\n",
      "Epoch 8 Loss 0.98897755 Acc 0.0\n",
      "batch 499 Loss 1.2233903 Acc 0.0\n",
      "batch 999 Loss 1.0470078 Acc 0.0\n",
      "batch 1499 Loss 0.39111894 Acc 1.0\n",
      "Epoch 9 Loss 0.39111894 Acc 1.0\n"
     ]
    }
   ],
   "source": [
    "initialize_all = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(initialize_all)\n",
    "for epoch in range(10):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = np.array([sample['w'] for sample in data]).reshape([batch_size,-1,300])\n",
    "        labels  = np.array([sample['y'] for sample in data]).reshape([batch_size,1])\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        if (batch+1) % 500 == 0:\n",
    "            print(\"batch\", batch, \"Loss\", l, \"Acc\", acc)\n",
    "    if epoch % 1 == 0:\n",
    "        print(\"Epoch\", epoch, \"Loss\", l, \"Acc\", acc)\n",
    "    random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.732\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_acc=0\n",
    "n=0\n",
    "for sample in test:\n",
    "    test_reviews = np.array([sample['w'] ]).reshape([1,-1,300])\n",
    "    test_labels  = np.array([sample['y']]).reshape([1,1])\n",
    "    test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "    test_acc += sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "    n+=1\n",
    "acc=test_acc/n \n",
    "print(\"Final accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final accuracy is not great, but it is _much higher_ than what we got with the MLP approach.  Playing around with the settings can properly improve this more (this really isn't that many epochs)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
